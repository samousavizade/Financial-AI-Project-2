{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "Random.seed!(1234);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = \"../data/assets.h5\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a stock return series to predict asset price moves\n",
    "\n",
    "To develop our trading strategy, we use the daily stock returns for some 995 US stocks for the eight year period from 2010 to 2017, and the features developed in Chapter 12 that include volatility and momentum factors as well as lagged returns with cross-sectional and sectoral rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"results\";\n",
    "\n",
    "if !isdir(results_path)\n",
    "    mkdir(results_path);\n",
    "end \n",
    "\n",
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, CSV;\n",
    "\n",
    "input = \"data.csv\";\n",
    "data = DataFrame(CSV.File(input));\n",
    "# data = data[1:10000, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = names(data[:, r\"fwd\"]);\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome= \"r01_fwd\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv = data[:, Not(outcomes)];\n",
    "y_cv = data[:, [\"symbol\", \"date\", outcome]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique(X_cv[:, \"symbol\"])\n",
    "X_cv[:, \"symbol\"] |> unique |> length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate model generation\n",
    "\n",
    "The following `make_model` function illustrates how to flexibly define various architectural elements for the search process. The dense_layers argument defines both the depth and width of the network as a list of integers. We also use dropout for regularization, expressed as a float in the range [0, 1] to define the probability that a given unit will be excluded from a training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "\n",
    "\n",
    "function make_model(dense_layers, input_dim, activation, dropout)\n",
    "    # '''\n",
    "    # input_dim: X_cv.shape[1]\n",
    "\n",
    "    # Creates a multi-layer perceptron model\n",
    "\n",
    "    # dense_layers: List of layer sizes; one number per layer\n",
    "    # '''\n",
    "\n",
    "    dense1_input_dim = input_dim;\n",
    "    dense1_output_dim = dense_layers[1];\n",
    "    dense1 = Flux.Dense(\n",
    "        dense1_input_dim,\n",
    "        dense1_output_dim,\n",
    "        activation);\n",
    "    \n",
    "    dense2_input_dim = dense1_output_dim;\n",
    "    dense2_output_dim = dense_layers[2];\n",
    "    dense2 = Flux.Dense(\n",
    "        dense2_input_dim,\n",
    "        dense2_output_dim,\n",
    "        activation);\n",
    "\n",
    "    dropout1 = Flux.Dropout(dropout);\n",
    "    dense3 = Flux.Dense(\n",
    "        dense2_output_dim,\n",
    "        1,\n",
    "        sigmoid);\n",
    "\n",
    "    model = Flux.Chain(\n",
    "        dense1,\n",
    "        dense2,\n",
    "        dropout1,\n",
    "        dense3);\n",
    "\n",
    "    return model;\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Flow Experiment Tracking\n",
    "\n",
    "MLflow is a platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. MLflow offers a set of lightweight APIs that can be used with any existing machine learning application or library (TensorFlow, PyTorch, XGBoost, etc), wherever you currently run ML code (e.g. in notebooks, standalone applications or the cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "\n",
    "mlflow = pyimport(\"mlflow\")\n",
    "\n",
    "MLF_EXPERIMENT_NAME = \"Optimizing a NN Architecture For Trading\"\n",
    "MLF_EXPERIMENT_ID = 0\n",
    "\n",
    "try\n",
    "    MLF_EXPERIMENT_ID = mlflow.get_experiment_by_name(MLF_EXPERIMENT_NAME).experiment_id\n",
    "catch e\n",
    "    MLF_EXPERIMENT_ID = mlflow.create_experiment(MLF_EXPERIMENT_NAME)\n",
    "end\n",
    "\n",
    "mlflow.set_experiment(experiment_id=MLF_EXPERIMENT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CV Parameters\n",
    "\n",
    "Now we just need to define our Keras classifier using the make_model function, set cross-validation (see chapter 6 on The Machine Learning Process and following for the OneStepTimeSeriesSplit), and the parameters that we would like to explore. \n",
    "\n",
    "We pick several one- and two-layer configurations, relu and tanh activation functions, and different dropout rates. We could also try out different optimizers (but did not run this experiment to limit what is already a computationally intensive effort):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_opts = [(16, 8), (32, 16), (32, 32), (64, 32)];\n",
    "activation_opts = [Flux.tanh];\n",
    "dropout_opts = [0, .1, .2];\n",
    "batch_size = [64, 256];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using IterTools, Random\n",
    "\n",
    "param_grid = IterTools.product(\n",
    "    dense_layer_opts,\n",
    "    activation_opts,\n",
    "    dropout_opts,\n",
    "    batch_size,\n",
    ") |> collect;\n",
    "\n",
    "parameters_grid = Random.shuffle(param_grid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model, x) = map(x -> x[1] - 1, argmax(model(x), dims=1));\n",
    "optimizer = Flux.RMSProp();\n",
    "epochs = 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, StatsBase, MLUtils, MLBase\n",
    "\n",
    "function performance_evaluation_dict(\n",
    "    ŷ,\n",
    "    y,\n",
    "    phase,\n",
    ")\n",
    "    ŷ, y = vec(ŷ), vec(y);\n",
    "\n",
    "    error_rate = MLBase.errorrate(ŷ, y);\n",
    "\n",
    "    accuracy = MLBase.correctrate(ŷ, y);\n",
    "\n",
    "    loss = Flux.binarycrossentropy(ŷ, y);\n",
    "\n",
    "    roc_nums::ROCNums = MLBase.roc(ŷ, y);\n",
    "\n",
    "    fpr = MLBase.false_positive_rate(roc_nums);\n",
    "    fnr = MLBase.false_negative_rate(roc_nums);\n",
    "\n",
    "    f1_score = MLBase.f1score(roc_nums);\n",
    "\n",
    "    metrics_dict = Dict(\n",
    "        \"$phase error_rate\" => error_rate,\n",
    "        \"$phase accuracy\" => accuracy,\n",
    "        \"$phase loss\" => loss,\n",
    "        \"$phase fpr\" => fpr,\n",
    "        \"$phase fnr\" => fnr,\n",
    "        \"$phase f1_score\" => f1_score,\n",
    "    );\n",
    "\n",
    "    metrics_dict\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_loop(train_data, model, optimiser, batch_size)\n",
    "    Flux.trainmode!(model)\n",
    "\n",
    "    for (x, y) ∈ eachobs(train_data, batchsize=batch_size)\n",
    "        # ... train supervised model on minibatches here\n",
    "        grads = gradient(Flux.params(model)) do\n",
    "            training_loss = Flux.binarycrossentropy(model(x), y)\n",
    "\n",
    "            # Code inserted here will be differentiated, unless you need that gradient information\n",
    "            # it is better to do the work outside this block.\n",
    "\n",
    "            return training_loss\n",
    "        end\n",
    "\n",
    "        # Insert whatever code you want here that needs training_loss, e.g. logging.\n",
    "        # logging_callback(training_loss)\n",
    "        # Insert what ever code you want here that needs gradient.\n",
    "        # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.\n",
    "\n",
    "        Flux.update!(optimiser, Flux.params(model), grads)\n",
    "\n",
    "        # Here you might like to check validation set accuracy, and break out to do early stopping.\n",
    "    end\n",
    "\n",
    "    X, Y = train_data\n",
    "\n",
    "    Y = map(x -> x[1] - 1, argmax(Y, dims=1))\n",
    "    Ŷ = predict(model, X)\n",
    "\n",
    "    Y, Ŷ\n",
    "end;\n",
    "\n",
    "function test_loop(test_data, model, optimiser, batch_size)\n",
    "    Flux.testmode!(model)\n",
    "\n",
    "    for (x, y) ∈ eachobs(test_data, batchsize=batch_size)\n",
    "        # # ... train supervised model on minibatches here\n",
    "        # grads = gradient(Flux.params(model)) do\n",
    "        #     training_loss = Flux.binarycrossentropy(model(x), y)\n",
    "\n",
    "        #     # Code inserted here will be differentiated, unless you need that gradient information\n",
    "        #     # it is better to do the work outside this block.\n",
    "\n",
    "        #     return training_loss\n",
    "        # end\n",
    "\n",
    "        # # Insert whatever code you want here that needs training_loss, e.g. logging.\n",
    "        # # logging_callback(training_loss)\n",
    "        # # Insert what ever code you want here that needs gradient.\n",
    "        # # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.\n",
    "\n",
    "        # Flux.update!(optimiser, Flux.params(model), grads)\n",
    "\n",
    "        # # Here you might like to check validation set accuracy, and break out to do early stopping.\n",
    "    end\n",
    "\n",
    "    X, Y = test_data;\n",
    "\n",
    "    Y = map(x -> x[1] - 1, argmax(Y, dims=1));\n",
    "\n",
    "    Y, Ŷ;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "We split the data into a training set for cross-validation, and keep the last 12 months with data as holdout test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 12;\n",
    "train_period_length = 21 * 12 * 4;\n",
    "test_period_length = 21 * 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ResumableFunctions\n",
    "\n",
    "struct MultipleTimeSeriesCV\n",
    "\n",
    "    n_splits::Int\n",
    "    train_pl::Int\n",
    "    test_pl::Int\n",
    "    lookahead\n",
    "    date_idx::String\n",
    "    shuffle::Bool\n",
    "\n",
    "    MultipleTimeSeriesCV(\n",
    "        n_splits::Int,\n",
    "        train_pl::Int,\n",
    "        test_pl::Int) = new(n_splits, train_pl, test_pl, 0, \"date\", true)\n",
    "\n",
    "end\n",
    "\n",
    "get_rows(data) = rownumber.(eachrow(data));\n",
    "\n",
    "@resumable function split(\n",
    "    cv::MultipleTimeSeriesCV,\n",
    "    X,\n",
    "    y=nothing)\n",
    "\n",
    "    n_splits = cv.n_splits\n",
    "    lookahead = cv.lookahead\n",
    "    test_length = cv.test_pl\n",
    "    train_length = cv.train_pl\n",
    "    shuffle = cv.shuffle\n",
    "    date_idx = cv.date_idx\n",
    "\n",
    "    unique_dates = X[:, \"date\"] |> unique;\n",
    "    days = sort(unique_dates, rev=true);\n",
    "\n",
    "    split_idx = [];\n",
    "    for i ∈ 1:n_splits\n",
    "        test_end_idx = i * test_length;\n",
    "        test_start_idx = test_end_idx + test_length;\n",
    "        train_end_idx = test_start_idx + lookahead - 1;\n",
    "        train_start_idx = train_end_idx + train_length + lookahead - 1;\n",
    "\n",
    "        push!(split_idx, (train_start_idx, train_end_idx, test_start_idx, test_end_idx));\n",
    "    end\n",
    "\n",
    "    dates = X[:, [date_idx]];\n",
    "\n",
    "    for (train_start, train_end, test_start, test_end) ∈ split_idx\n",
    "        mask = (dates[:, date_idx] .> days[train_start]) .& (dates[:, date_idx] .< days[train_end]);\n",
    "        train_idx = get_rows(dates[mask, :]);\n",
    "\n",
    "        mask = (dates[:, date_idx] .> days[test_start]) .& (dates[:, date_idx] .< days[test_end]);\n",
    "        test_idx = get_rows(dates[mask, :]);\n",
    "\n",
    "        if shuffle Random.shuffle(train_idx) end\n",
    "\n",
    "        @yield (train_idx, test_idx)\n",
    "    end\n",
    "end;\n",
    "\n",
    "function get_n_splits(cv)\n",
    "    cv.n_splits\n",
    "end;\n",
    "\n",
    "cv = MultipleTimeSeriesCV(\n",
    "    n_splits,\n",
    "    train_period_length,\n",
    "    test_period_length,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BSON Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = DataFrame();\n",
    "mlflow.end_run()\n",
    "\n",
    "for parameters ∈ parameters_grid\n",
    "    dense_layers, activation, dropout, batch_size = parameters\n",
    "\n",
    "    start_time = time()\n",
    "    for (fold, (train_data_idx, val_data_idx)) ∈ enumerate(split(cv, X_cv))\n",
    "\n",
    "        mlflow.start_run(run_name=\"Fold $(fold)\");\n",
    "\n",
    "        index_columns = [\"symbol\", \"date\"]\n",
    "\n",
    "        x_train, y_train = X_cv[train_data_idx, Not(index_columns)] |> Matrix, y_cv[train_data_idx, :]\n",
    "        x_val, y_val = X_cv[val_data_idx, Not(index_columns)] |> Matrix, y_cv[val_data_idx, :]\n",
    "\n",
    "        preds = rename(y_val, \"r01_fwd\" => \"actual\")\n",
    "\n",
    "        r = DataFrames.combine(DataFrames.groupby(y_val, \"date\"), \"date\" .=> length)\n",
    "\n",
    "        input_dim = size(x_train, 2)\n",
    "\n",
    "        model = make_model(\n",
    "            dense_layers,\n",
    "            input_dim,\n",
    "            activation,\n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "        indices_train = y_train[:, index_columns]\n",
    "        indices_val = y_val[:, index_columns]\n",
    "\n",
    "        y_train = y_train[:, Not(index_columns)] |> Matrix\n",
    "        y_val = y_val[:, Not(index_columns)] |> Matrix\n",
    "\n",
    "        # Note that Transpose is needed to make the model fit the data.\n",
    "        train_data = (x_train', y_train')\n",
    "        train_loader = DataLoader(train_data, batchsize=batch_size)\n",
    "        for epoch = 1:10\n",
    "            print(\"Epoch: $epoch, \");\n",
    "            \n",
    "            evalcb = Flux.throttle(15) do\n",
    "                # Show loss\n",
    "                BSON.@save \"model-checkpoint.bson\" model\n",
    "            end\n",
    "\n",
    "            loss(x, y) = Flux.mse(model(x), y);\n",
    "            Flux.train!(loss, Flux.params(), train_loader, optimiser, cb=evalcb);\n",
    "            ŷ_val = model(x_val');\n",
    "\n",
    "            preds[:, \"$epoch\"] = vec(ŷ_val');\n",
    "\n",
    "            x = DataFrames.combine(\n",
    "                DataFrames.groupby(preds, \"date\"),\n",
    "                [\"actual\", \"$epoch\"] => ((a, e) -> (StatsBase.corspearman(a, e))) => \"$epoch\");\n",
    "\n",
    "            r[:, \"$epoch\"] = x[:, \"$epoch\"];\n",
    "        end\n",
    "\n",
    "        r[:, \"dense_layers\"] .= string(dense_layers);\n",
    "        r[:, \"activation\"] .= string(activation);\n",
    "        r[:, \"dropout\"] .= dropout;\n",
    "        r[:, \"batch_size\"] .= batch_size;\n",
    "        r[:, \"fold\"] .= fold;\n",
    "        \n",
    "        append!(ic, r);\n",
    "\n",
    "        mlflow.end_run();\n",
    "    end\n",
    "    end_time = time();\n",
    "\n",
    "    process_time = end_time - start_time;\n",
    "    print(\"$process_time \\n\");\n",
    "end\n",
    "\n",
    "output = \"scores.csv\";\n",
    "CSV.write(output, ic);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
