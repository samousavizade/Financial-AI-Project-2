{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use backpropagation to train a feedforward NN\n",
    "\n",
    "This noteboo implements a simple single-layer architecture and forward propagation computations using julia libraries.\n",
    "\n",
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA, Flux, Random\n",
    "\n",
    "Random.seed!(1234);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"results\";\n",
    "!isdir(results_path) && mkdir(results_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "\n",
    "### Generate random data\n",
    "\n",
    "The target <code>y</code> represents two classes generated by two circular distribution that are not linearly separable because class 0 surrounds class 1.\n",
    "\n",
    "We will generate 50,000 random samples in the form of two concentric circles with different radius using scikit-learn's make_circles function so that the classes are not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000;\n",
    "factor = 0.1;\n",
    "noise = 0.1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 5000;\n",
    "learning_rate = 0.0001;\n",
    "momentum_factor = 0.5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted From SyntheticDatasets\n",
    "using DataFrames\n",
    "using PyCall\n",
    "sk = pyimport(\"sklearn.datasets\")\n",
    "\n",
    "function convert(features::Array{T, 2}, labels::Array{D, 1})::DataFrame where {T <: Number, D <: Number}\n",
    "    df = DataFrame()\n",
    "\n",
    "    for i = 1:size(features)[2]\n",
    "        df[!, Symbol(\"feature_$(i)\")] = eltype(features)[]\n",
    "    end\n",
    "    \n",
    "    df[!, :label] = eltype(labels)[]\n",
    "    \n",
    "    for label in unique(labels)\n",
    "        for i in findall(r->r == label, labels)\n",
    "            push!(df, (features[i, :]... , label))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return df\n",
    "end\n",
    "\n",
    "function convert(features::Array{T, 2}, labels::Array{D, 2})::DataFrame where {T <: Number, D <: Number}\n",
    "    df = DataFrame()\n",
    "\n",
    "    for i = 1:size(features)[2]\n",
    "        df[!, Symbol(\"feature_$(i)\")] = eltype(features)[]\n",
    "    end\n",
    "\n",
    "    for i = 1:size(labels)[2]\n",
    "        df[!, Symbol(\"label_$(i)\")] = eltype(labels)[]\n",
    "    end\n",
    "    \n",
    "    for row in 1:size(features)[1]\n",
    "        push!(df, (features[row, :]... , labels[row, :]...))\n",
    "    end\n",
    "    \n",
    "    return df\n",
    "end\n",
    "\n",
    "\n",
    "(features, labels) = sk.make_circles(\n",
    "    n_samples=N,\n",
    "    shuffle=true,\n",
    "    noise=noise,\n",
    "    factor=factor\n",
    ");\n",
    "\n",
    "circles = convert(features, labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_column_name = \"label\"\n",
    "\n",
    "y = circles[:, [labels_column_name]];\n",
    "y = Matrix(y);\n",
    "\n",
    "X = select!(circles, Not(labels_column_name));\n",
    "X = Matrix(X);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define outcome matrix\n",
    "Y = zeros(Float64, (N, 2));\n",
    "for c ∈ [0, 1]\n",
    "    # mask = collect(Iterators.flatten(y .== c))\n",
    "    mask = vec(y .== c)\n",
    "    Y[mask, c+1] .= 1.0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Shape of: X: $(size(X)) | Y: $(size(Y)) | y: $(size(y))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PlotlyJS\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set PlotlyJS Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.default = \"plotly_dark\";\n",
    "PlotlyJS.templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(\n",
    "    X1=X[:, 1],\n",
    "    X2=X[:, 2],\n",
    "    Label=y[:, 1];\n",
    ");\n",
    "\n",
    "ax = PlotlyJS.plot(\n",
    "    df,\n",
    "    x=:X1,\n",
    "    y=:X2,\n",
    "    m=1,\n",
    "    color=:Label,\n",
    "    kind=\"scatter\",\n",
    "    mode=\"markers\",\n",
    "    labels=Dict(\n",
    "        :X1 => \"X\",\n",
    "        :X2 => \"Y\",\n",
    "        :Label => \"Labels\"\n",
    "    ),\n",
    "    marker=attr(size=8, line=attr(width=1, color=\"DarkSlateGrey\")),\n",
    "    PlotlyJS.Layout(\n",
    "        title=\"Circles Dataset Visualization\",\n",
    "        width=600, height=600,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "### Hidden Layer Activations\n",
    "\n",
    "The hidden layer $h$ projects the 2D input into a 3D space. To this end, the hidden layer weights are a $2\\times3$ matrix $\\mathbf{W}^h$, and the hidden layer bias vector $\\mathbf{b}^h$ is a 3-dimensional vector:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\scriptscriptstyle 2 \\times 3}{\\mathbf{W}^h} =\n",
    "\\begin{bmatrix} \n",
    "w^h_{11} & w^h_{12} & w^h_{13} \\\\\n",
    "w^h_{21} & w^h_{22} & w^h_{23}\n",
    "\\end{bmatrix}\n",
    "&& \\underset{\\scriptscriptstyle 1 \\times 3}{\\mathbf{b}^h} = \n",
    "\\begin{bmatrix} \n",
    "b^h_1 & b^h_2 & b^h_3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The output layer values $\\mathbf{Z}^h$ result from the dot product of the $N\\times\\ 2$ input data $\\mathbf{X}$ and the the $2\\times3$ weight matrix $\\mathbf{W}^h$ and the addition of the $1\\times3$ hidden layer bias vector $\\mathbf{b}^h$:\n",
    "\n",
    "$$\\underset{\\scriptscriptstyle N \\times 3}{\\mathbf{Z}^h} = \\underset{\\scriptscriptstyle N \\times 2}{\\vphantom{\\mathbf{W}^o}\\mathbf{X}}\\cdot\\underset{\\scriptscriptstyle 2 \\times 3}{\\mathbf{W}^h} + \\underset{\\scriptscriptstyle 1 \\times 3}{\\mathbf{b}^h}$$\n",
    "\n",
    "The logistic sigmoid function $\\sigma$ applies a non-linear transformation to $\\mathbf{Z}^h$ to yield  the hidden layer activations as an $N\\times3$ matrix:\n",
    "\n",
    "$$\\underset{\\scriptscriptstyle N \\times 3}{\\mathbf{H}} = \\sigma(\\mathbf{X} \\cdot \\mathbf{W}^h + \\mathbf{b}^h) = \\frac{1}{1+e^{−(\\mathbf{X} \\cdot \\mathbf{W}^h + \\mathbf{b}^h)}} = \\begin{bmatrix} \n",
    "h_{11} & h_{12} & h_{13} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "h_{N1} & h_{N2} & h_{N3}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function logistic(z)\n",
    "    1 / (1 + exp(-z))\n",
    "end;\n",
    "\n",
    "function hidden_layer(input_data, weights, bias)\n",
    "    L = input_data * weights .+ bias\n",
    "    result = logistic.(L)\n",
    "    result\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Activations\n",
    "\n",
    "The values $\\mathbf{Z}^o$ for the output layer $o$ are a $N\\times2$ matrix that results from the dot product of the $\\underset{\\scriptscriptstyle N \\times 3}{\\mathbf{H}}$ hidden layer activation matrix with the $3\\times2$ output weight matrix $\\mathbf{W}^o$ and the addition of the $1\\times2$ output bias vector $\\mathbf{b}^o$:\n",
    "\n",
    "$$\\underset{\\scriptscriptstyle N \\times 2}{\\mathbf{Z}^o} = \\underset{\\scriptscriptstyle N \\times 3}{\\vphantom{\\mathbf{W}^o}\\mathbf{H}}\\cdot\\underset{\\scriptscriptstyle 3 \\times 2}{\\mathbf{W}^o} + \\underset{\\scriptscriptstyle 1 \\times 2}{\\mathbf{b}^o}$$\n",
    "\n",
    "The Softmax function $\\varsigma$ squashes the unnormalized probabilities predicted for each class  to lie within $[0, 1]$ and sum to 1.  The result is a $N\\times2$ matrix with one column for each output class.\n",
    "\n",
    "$$\\underset{\\scriptscriptstyle N \\times 2}{\\mathbf{\\hat{Y}}} \n",
    "= \\varsigma(\\mathbf{H} \\cdot \\mathbf{W}^o + \\mathbf{b}^o)\n",
    "= \\frac{e^{Z^o}}{\\sum_{c=1}^C e^{\\mathbf{z}^o_c}}\n",
    "= \\frac{e^{H \\cdot W^o + \\mathbf{b}^o}}{\\sum_{c=1}^C e^{H \\cdot \\mathbf{w}^o_c + b^o_c}}\n",
    "= \\begin{bmatrix} \n",
    "\\hat{y}_{11} & \\hat{y}_{12}\\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\hat{y}_{n1} & \\hat{y}_{n2} \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softmax(X)\n",
    "    exp.(X) ./ sum(exp.(X), dims=2)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function output_layer(hidden_activations, weights, bias)\n",
    "    softmax(hidden_activations * weights .+ bias)\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "The `forward_prop` function combines the previous operations to yield the output activations from the  input data as a function of weights and biases. The `predict` function produces the binary class predictions given weights, biases, and input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forward_prop(\n",
    "    data,\n",
    "    hidden_weights,\n",
    "    hidden_bias,\n",
    "    output_weights,\n",
    "    output_bias)\n",
    "\n",
    "    hidden_activations = hidden_layer(data, hidden_weights, hidden_bias)\n",
    "    output_activations = output_layer(hidden_activations, output_weights, output_bias)\n",
    "    output_activations\n",
    "end;\n",
    "\n",
    "function forward_prop(\n",
    "    data,\n",
    "    wab)\n",
    "\n",
    "    hidden_weights, hidden_bias, output_weights, output_bias = wab\n",
    "\n",
    "    hidden_activations = hidden_layer(data, hidden_weights, hidden_bias)\n",
    "    output_activations = output_layer(hidden_activations, output_weights, output_bias)\n",
    "    output_activations\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(\n",
    "    data,\n",
    "    hidden_weights,\n",
    "    hidden_bias,\n",
    "    output_weights,\n",
    "    output_bias)\n",
    "\n",
    "    y_predicted_proba = forward_prop(\n",
    "        data,\n",
    "        hidden_weights,\n",
    "        hidden_bias,\n",
    "        output_weights,\n",
    "        output_bias\n",
    "    )\n",
    "\n",
    "    y_predicted_proba_rounded = round.(y_predicted_proba)\n",
    "\n",
    "    y_predicted_proba_rounded\n",
    "end;\n",
    "\n",
    "function predict(\n",
    "    data,\n",
    "    trained_params)\n",
    "\n",
    "    hidden_weights, hidden_bias, output_weights, output_bias = trained_params\n",
    "\n",
    "    y_predicted_proba = forward_prop(\n",
    "        data,\n",
    "        hidden_weights,\n",
    "        hidden_bias,\n",
    "        output_weights,\n",
    "        output_bias\n",
    "    )\n",
    "\n",
    "    y_predicted_proba_rounded = round.(y_predicted_proba)\n",
    "\n",
    "    y_predicted_proba_rounded\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss\n",
    "\n",
    "The cost function $J$ uses the cross-entropy loss $\\xi$ that sums the deviations of the predictions for each class $c$  $\\hat{y}_{ic}, i=1,...,N$ from the actual outcome.\n",
    "\n",
    "$$J(\\mathbf{Y},\\mathbf{\\hat{Y}}) = \\sum_{i=1}^n \\xi(\\mathbf{y}_i,\\mathbf{\\hat{y}}_i) = − \\sum_{i=1}^N \\sum_{i=c}^{C} y_{ic} \\cdot log(\\hat{y}_{ic})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(ŷ, y)\n",
    "    # Binary Cross Entropy Loss\n",
    "    # loss_values = -(y_true .* log.(y_hat)) .- ((1 .- y_true) .* log.(1 .- y_hat));\n",
    "    loss_values = -(y .* log.(ŷ))\n",
    "    sum(loss_values)\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Backpropagation updates parameters values based on the partial derivative of the loss with respect to that parameter, computed using the chain rule.\n",
    "\n",
    "### Loss Function Gradient\n",
    "\n",
    "The derivative of the loss function $J$ with respect to each output layer activation $\\varsigma(\\mathbf{Z}^o_i), i=1,...,N$, is a very simple expression:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial z^0_i} = \\delta^o = \\hat{y}_i-y_i$$\n",
    "\n",
    "See [here](https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function) and [here](https://deepnotes.io/softmax-crossentropy) for details on derivation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_gradient(ŷᵢ, yᵢ)\n",
    "    ŷᵢ - yᵢ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer Gradients\n",
    "\n",
    "#### Output Weight Gradients\n",
    "\n",
    "To propagate the updates back to the output layer weights, we take the partial derivative of the loss function with respect to the weight matrix:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^o} = H^T \\cdot (\\mathbf{\\hat{Y}}-\\mathbf{Y}) = H^T \\cdot \\delta^{o}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function output_weight_gradient(H, loss_grad)\n",
    "    H' * loss_grad\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Bias Update\n",
    "\n",
    "To update the output layer bias values, we similarly apply the chain rule to obtain the partial derivative of the loss function with respect to the bias vector:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b}_{o}} \n",
    "= \\frac{\\partial \\xi}{\\partial \\mathbf{\\hat{Y}}} \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}^o}  \\frac{\\partial \\mathbf{Z}^{o}}{\\partial \\mathbf{b}^o}\n",
    "= \\sum_{i=1}^N 1 \\cdot (\\mathbf{\\hat{y}}_i - \\mathbf{y}_i) \n",
    "= \\sum_{i=1}^N \\delta_{oi}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function output_bias_gradient(loss_grad)\n",
    "    sum(loss_grad, dims=1)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer gradients\n",
    "\n",
    "$$\\delta_{h} \n",
    "= \\frac{\\partial J}{\\partial \\mathbf{Z}^h} \n",
    "= \\frac{\\partial J}{\\partial \\mathbf{H}} \\frac{\\partial \\mathbf{H}}{\\partial \\mathbf{Z}^h} \n",
    "= \\frac{\\partial J}{\\partial \\mathbf{Z}^o} \\frac{\\partial \\mathbf{Z}^o}{\\partial H} \\frac{\\partial H}{\\partial \\mathbf{Z}^h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hidden_layer_gradient(H, out_weights, loss_grad)\n",
    "    H .* (1 .- H) .* (loss_grad * out_weights')\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Weight Gradient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^h} = \\mathbf{X}^T \\cdot \\delta^{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hidden_weight_gradient(X, hidden_layer_grad)\n",
    "    X' * hidden_layer_grad\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Bias Gradient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\xi}{\\partial \\mathbf{b}_{h}} \n",
    "= \\frac{\\partial \\xi}{\\partial H} \\frac{\\partial H}{\\partial Z_{h}} \\frac{\\partial Z_{h}}{\\partial \\mathbf{b}_{h}}\n",
    "= \\sum_{j=1}^N \\delta_{hj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hidden_bias_gradient(hidden_layer_grad)\n",
    "    sum(hidden_layer_grad, dims=1)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_weights()\n",
    "    hidden_weights = randn(2, 3)\n",
    "    hidden_bias = randn(1, 3)\n",
    "    output_weights = randn(3, 2)\n",
    "    output_bias = randn(1, 2)\n",
    "    # hidden_weights, hidden_bias, output_weights, output_bias\n",
    "    [hidden_weights, hidden_bias, output_weights, output_bias]\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_gradients(X, y_true, w_h, b_h, w_o, b_o)\n",
    "    hidden_activations = hidden_layer(X, w_h, b_h)\n",
    "    y_hat = output_layer(hidden_activations, w_o, b_o)\n",
    "\n",
    "    loss_grad = loss_gradient(y_hat, y_true)\n",
    "    output_weight_grad = output_weight_gradient(hidden_activations, loss_grad)\n",
    "    output_bias_grad = output_bias_gradient(loss_grad)\n",
    "\n",
    "    hidden_layer_grad = hidden_layer_gradient(hidden_activations, w_o, loss_grad)\n",
    "    hidden_weight_grad = hidden_weight_gradient(X, hidden_layer_grad)\n",
    "    hidden_bias_grad = hidden_bias_gradient(hidden_layer_grad)\n",
    "\n",
    "    [hidden_weight_grad, hidden_bias_grad, output_weight_grad, output_bias_grad]\n",
    "end;\n",
    "\n",
    "function compute_gradients(X, y_true, wab)\n",
    "    w_h, b_h, w_o, b_o = wab\n",
    "    hidden_activations = hidden_layer(X, w_h, b_h)\n",
    "    y_hat = output_layer(hidden_activations, w_o, b_o)\n",
    "\n",
    "    loss_grad = loss_gradient(y_hat, y_true)\n",
    "    out_weight_grad = output_weight_gradient(hidden_activations, loss_grad)\n",
    "    out_bias_grad = output_bias_gradient(loss_grad)\n",
    "\n",
    "    hidden_layer_grad = hidden_layer_gradient(hidden_activations, w_o, loss_grad)\n",
    "    hidden_weight_grad = hidden_weight_gradient(X, hidden_layer_grad)\n",
    "    hidden_bias_grad = hidden_bias_gradient(hidden_layer_grad)\n",
    "\n",
    "    hidden_weight_grad, hidden_bias_grad, out_weight_grad, out_bias_grad\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Gradients\n",
    "\n",
    "It's easy to make mistakes with the numerous inputs to the backpropagation algorithm. A simple way to test for accuracy is to compare the change in the output for slightly perturbed parameter values with the change implied by the computed gradient (see [here](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization) for more detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change individual parameters by +/- epsilon\n",
    "ϵ = 1e-4;\n",
    "\n",
    "# initialize weights and biases\n",
    "params = initialize_weights();\n",
    "\n",
    "# Get all parameter gradients\n",
    "grad_params = compute_gradients(X, Y, params);\n",
    "\n",
    "# Check each parameter matrix\n",
    "for (i, param) ∈ enumerate(params)\n",
    "    print(\"round $i\\n\")\n",
    "    n_rows, n_cols = size(param)\n",
    "    for row ∈ 1:n_rows\n",
    "        for col ∈ 1:n_cols\n",
    "            params_low = deepcopy(params)\n",
    "            params_low[i][row, col] -= ϵ\n",
    "\n",
    "            params_high = deepcopy(params)\n",
    "            params_high[i][row, col] += ϵ\n",
    "\n",
    "            # Compute the numerical gradient\n",
    "            loss_high = loss(forward_prop(X, params_high), Y)\n",
    "            loss_low = loss(forward_prop(X, params_low), Y)\n",
    "\n",
    "            numerical_gradient = (loss_high - loss_low) / 2ϵ\n",
    "            backprop_gradient = grad_params[i][row, col]\n",
    "\n",
    "            print(\"numerical_gradient: $numerical_gradient \\nbackprop_gradient $backprop_gradient \\n\\n\")\n",
    "\n",
    "            @assert isapprox(numerical_gradient, backprop_gradient)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_momentum(\n",
    "    X,\n",
    "    y_true,\n",
    "    param_list,\n",
    "    Ms,\n",
    "    momentum_term,\n",
    "    learning_rate)\n",
    "    \"\"\"Update the momentum matrices.\"\"\"\n",
    "\n",
    "    gradients = compute_gradients(X, y_true, param_list)\n",
    "    [momentum_term .* momentum .- learning_rate .* grads for (momentum, grads) in zip(Ms, gradients)]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_params(param_list, Ms)\n",
    "    \"\"\"Update the parameters.\"\"\"\n",
    "    [P .+ M for (P, M) in zip(param_list, Ms)]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_network(; iterations=1000, lr=0.01, mf=0.1)\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    param_list = initialize_weights()\n",
    "\n",
    "    # Momentum Matrices = [MWh, Mbh, MWo, Mbo]\n",
    "    Ms = [zeros(Float64, size(M)) for M ∈ param_list]\n",
    "\n",
    "    train_loss = [loss(forward_prop(X, param_list), Y)]\n",
    "    for i ∈ 1:iterations\n",
    "        if mod(i, 1000) == 0\n",
    "            print(\"iteration $i ... \\n\")\n",
    "        end\n",
    "        # Update the moments and the parameters\n",
    "        Ms = update_momentum(X, Y, param_list, Ms, mf, lr)\n",
    "\n",
    "        param_list = update_params(param_list, Ms)\n",
    "\n",
    "        append!(train_loss, loss(forward_prop(X, param_list), Y))\n",
    "    end\n",
    "\n",
    "    param_list, train_loss\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_params, train_loss = train_network(\n",
    "    iterations=n_iterations,\n",
    "    lr=learning_rate,\n",
    "    mf=momentum_factor);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights, hidden_bias, output_weights, output_bias = trained_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Loss\n",
    "\n",
    "This plot displays the training loss over 50K iterations for 50K training samples with a momentum term of 0.5 and a learning rate of 1e-4. \n",
    "\n",
    "It shows that it takes over 5K iterations for the loss to start to decline but then does so very fast. We have not uses stochastic gradient descent, which would have likely significantly accelerated convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_per_iteration_df = DataFrame(\n",
    "    Iteration=collect(1:n_iterations+1),\n",
    "    LogLoss=log.(train_loss),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ax = PlotlyJS.plot(\n",
    "#     train_loss_per_iteration_df,\n",
    "#     x=:Iteration,\n",
    "#     y=:LogLoss,\n",
    "#     m=1,\n",
    "#     kind=\"scatter\",\n",
    "#     mode=\"lines\",\n",
    "#     labels=Dict(\n",
    "#         :Iteration => \"Iteration\",\n",
    "#         :LogLoss => \"Log(ξ)\",\n",
    "#     ),\n",
    "#     PlotlyJS.Layout(\n",
    "#         title=\"Log(ξ) / Iteration\",\n",
    "#         width=800, height=500,\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary\n",
    "\n",
    "The following plots show the function learned by the neural network with a three-dimensional hidden layer form two-dimensional data with two classes that are not linearly separable as shown on the left. The decision boundary misclassifies very few data points and would further improve with continued training. \n",
    "\n",
    "The second plot shows the representation of the input data learned by the hidden layer. The network learns hidden layer weights so that the projection of the input from two to three dimensions enables the linear separation of the two classes. \n",
    "\n",
    "The last plot shows how the output layer implements the linear separation in the form of a cutoff value of 0.5 in the output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function meshgrid(x, y)\n",
    "    mg = Iterators.product(x, y)\n",
    "\n",
    "    xx = first.(mg)\n",
    "    yy = last.(mg)\n",
    "\n",
    "    xx, yy\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 instead of 200 for more resulotion boundry in the coutour plot.\n",
    "n_vals = 500;\n",
    "x1 = range(-1.5, 1.5, n_vals);\n",
    "x2 = range(-1.5, 1.5, n_vals);\n",
    "# create the grid\n",
    "xx, yy = meshgrid(x1, x2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fill the feature space\n",
    "feature_space = zeros(n_vals, n_vals);\n",
    "for i ∈ 1:n_vals\n",
    "    for j ∈ 1:n_vals\n",
    "        X_ = [xx[i, j] yy[i, j]]\n",
    "        probabilities = predict(X_, trained_params)\n",
    "        index = argmax(probabilities)\n",
    "        feature_space[i, j] = index[2] - 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform(df, :Label => ByRow(label -> label == 0 ? \"Outer Circle\" : \"Inner Circle\") => :State);\n",
    "\n",
    "alpha = 0.15;\n",
    "colorscale = [[0, \"rgba(0,0,255,$alpha)\"], [1, \"rgba(255,255,0,$alpha)\"]]\n",
    "\n",
    "PlotlyJS.plot([\n",
    "        contour(\n",
    "            x=x1, # horizontal axis\n",
    "            y=x2, # vertical axis\n",
    "            z=feature_space', # data\n",
    "\n",
    "            # contours_coloring=\"lines\",\n",
    "\n",
    "            # autocolorscale=true,\n",
    "            colorscale=colorscale,\n",
    "\n",
    "            line_width=1,\n",
    "            line_smoothing=0.85,\n",
    "            colorbar=attr(\n",
    "                title=\"Decision Boundry\", # title here\n",
    "                titleside=\"right\",\n",
    "                titlefont=attr(\n",
    "                    size=14,\n",
    "                    family=\"Arial, sans-serif\"),\n",
    "                thickness=25,\n",
    "                thicknessmode=\"pixels\",\n",
    "                len=0.8,\n",
    "                lenmode=\"fraction\",\n",
    "                outlinewidth=0,\n",
    "            ),\n",
    "\n",
    "            # Smooth Coloring based on Z (In this case Z = 0, 1)\n",
    "            contours=attr(\n",
    "                coloring =\"heatmap\",\n",
    "                # showlabels = true, # show labels on contours\n",
    "                labelfont = attr( # label font properties\n",
    "                    size = 12,\n",
    "                    color = \"white\",\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        PlotlyJS.scatter(\n",
    "            df,\n",
    "            x=:X1,\n",
    "            y=:X2,\n",
    "\n",
    "            # NOTE: marker_color for scatter() and color for plot()\n",
    "            marker_color=:Label,\n",
    "            \n",
    "            text=:State,\n",
    "            \n",
    "            mode=\"markers\",\n",
    "            labels=Dict(\n",
    "                :X1 => \"X\",\n",
    "                :X2 => \"Y\",\n",
    "                :Label => \"Labels\"\n",
    "            ),\n",
    "            marker=attr(size=8, line=attr(width=1, color=\"DarkSlateGrey\")),\n",
    "        ),\n",
    "        ],\n",
    "    PlotlyJS.Layout(\n",
    "        title=\"Circles Dataset Visualization\",\n",
    "        width=700, height=700,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection on Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 instead of 200 for more resulotion boundry in the coutour plot.\n",
    "n_vals = 25;\n",
    "x1 = range(-1.5, 1.5, n_vals);\n",
    "x2 = range(-1.5, 1.5, n_vals);\n",
    "# create the grid\n",
    "xx, yy = meshgrid(x1, x2);\n",
    "\n",
    "X_ = [vec(xx) vec(yy)]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ = hidden_layer(X, hidden_weights, hidden_bias);\n",
    "\n",
    "hidden_layer_output_df = DataFrame(\n",
    "    X=h_[:, 1],\n",
    "    Y=h_[:, 2],\n",
    "    Z=h_[:, 3],\n",
    "    Label=y[:, 1],\n",
    ");\n",
    "\n",
    "hidden_layer_output_df = transform(hidden_layer_output_df,\n",
    "    :Label => ByRow(label -> label == 0 ? \"Outer Circle\" : \"Inner Circle\") => :State\n",
    ");\n",
    "\n",
    "plot(\n",
    "    PlotlyJS.scatter(\n",
    "        hidden_layer_output_df,\n",
    "        x=:X,\n",
    "        y=:Y,\n",
    "        z=:Z,\n",
    "\n",
    "        # NOTE: marker_color for scatter() and color for plot()\n",
    "        marker_color=:Label, type=\"scatter3d\", text=:State,\n",
    "        mode=\"markers\",\n",
    "        labels=Dict(\n",
    "            :Label => \"Labels\"\n",
    "        ),\n",
    "        marker=attr(size=8, line=attr(width=1, color=\"DarkSlateGrey\")),\n",
    "    ),\n",
    "    Layout(\n",
    "        title=\"Hidden Layers Outputs\",\n",
    "        width=700, height=700,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Output Surface Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = forward_prop(X_', hidden_weights, hidden_bias, output_weights, output_bias);\n",
    "zz = reshape(zz[:, 2], 25, 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    PlotlyJS.surface(\n",
    "        z=zz,\n",
    "        x=xx,\n",
    "        y=yy,\n",
    "        contours_z=attr(\n",
    "            show=true,\n",
    "            usecolormap=true,\n",
    "            highlightcolor=\"limegreen\",\n",
    "            project_z=true\n",
    "        ),),\n",
    "    Layout(\n",
    "        title=\"Mt Bruno Elevation\", autosize=false,\n",
    "        width=700, height=700,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "To sum up: we have seen how a very simple network with a single hidden layer with three nodes and a total of 17 parameters is able to learn how to solve a non-linear classification problem using backprop and gradient descent with momentum. \n",
    "\n",
    "We will next review key design choices useful to design and train more complex architectures before we turn to popular deep learning libraries that facilitate the process by providing many of these building blocks and automating the differentiation process to compute the gradients and implement backpropagation."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "6d9ce44b-e756-46dc-a706-44122f9a16c1",
   "lastKernelId": "f7e5a288-1b6f-40cb-a3fc-81d2cf21296c"
  },
  "interpreter": {
   "hash": "98a2e55eb24693f978cc00eb2290e3b206f5527802399b9f72e816e262c791d9"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
